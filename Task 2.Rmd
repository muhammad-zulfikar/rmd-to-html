---
title: "Sentiment Analysis between Donald Trump and Hillary Clinton"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 1
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	cache = TRUE
)
```

<br>

```
This task has two parts.

In the first part, we will use a dataset called tweet.csv that contains tweets and comments from Donald Trump and Hillary Clinton. We will analyze the sentiments in these tweets and comments. We will categorize the tweets and comments as positive, negative, or neutral and create visualizations to show the results.

In the second part, we will read journal articles about the sentiments around Hillary Clinton and Donald Trump during the 2016 U.S. Presidential Election. We will analyze these articles to understand how sentiment analysis was used and what trends were found in public opinion.
```

<br>

# Sentiment Analysis from Tweets 

<br>

## Load required libraries

```{r warning=FALSE, message=FALSE}
# Load required libraries
library(readr)
library(dplyr)
library(stringr)
library(tidytext)
library(ggplot2)
```

<br>

## Read the CSV file

```{r warning=FALSE, message=FALSE}
# Read the CSV file
tweets <- read_csv("./resources/tweets.csv")
```

<br>

## Data cleaning and preprocessing

```{r warning=FALSE, message=FALSE}
# Data cleaning and preprocessing
tweets <- tweets %>%
  mutate(text = str_replace_all(text, "https?://\\S+", "")) %>%  # Remove URLs
  mutate(text = str_replace_all(text, "@\\w+", "")) %>%  # Remove mentions
  mutate(text = str_replace_all(text, "#\\w+", "")) %>%  # Remove hashtags
  mutate(text = str_replace_all(text, "[^[:alnum:][:space:]]+", "")) %>%  # Remove special characters
  mutate(text = str_trim(text)) %>%  # Trim whitespace
  filter(!is.na(text) & text != "")  # Remove empty/NA rows
```

```{r warning=FALSE, message=FALSE}
# Tokenize text data
tweets_tokens <- tweets %>%
  unnest_tokens(word, text)
```

<br>

## Sentiment Analysis

```{r warning=FALSE, message=FALSE}
# Perform sentiment analysis
sentiment_scores <- tweets_tokens %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(handle) %>%
  summarize(sentiment_score = sum(value))
```

```{r warning=FALSE, message=FALSE}
# Plot sentiment analysis graph
ggplot(sentiment_scores, aes(x = handle, y = sentiment_score, fill = handle)) +
  geom_bar(stat = "identity") +
  labs(title = "Sentiment Analysis between Donald Trump and Hillary Clinton",
       x = "Candidate",
       y = "Sentiment Score") +
  theme_minimal() +
  theme(legend.position = "none")
```

<br>

# Sentiment Analysis from Journal/Article

<br>

## Data Scraping

<br>

### Load necessary libraries

```{r warning=FALSE, message=FALSE}
# Load necessary libraries
library(rvest)
library(httr)
library(tools)
```

<br>

### Scrape journal/article PDFs from Google Scholar

```{r warning=FALSE, message=FALSE}
# Function to scrape Google Scholar and download PDF files
scrape_google_scholar <- function(query, pages, output_dir) {

  # Create output directories if they do not exist
  if (!dir.exists(output_dir)) {
    dir.create(output_dir, recursive = TRUE)
  }

  # File to store metadata
  metadata_file <- file.path(output_dir, "pdf_metadata.csv")

  # Initialize metadata storage
  metadata <- data.frame(Link = character(), Title = character(), stringsAsFactors = FALSE)

  # List to track downloaded PDF links
  downloaded_links <- c()
  file_counter <- 1

  # Loop through each page
  for (i in seq(0, (pages - 1) * 10, by = 10)) {

    # Construct the URL
    url <- paste0("https://scholar.google.com/scholar?start=", i, "&q=", query, "&hl=en&as_sdt=0,5")

    # Read the page content
    page <- read_html(url)

    # Extract PDF links
    links <- page %>% html_nodes("a") %>% html_attr("href")

    # Filter PDF links
    pdf_links <- links[grepl("\\.pdf$", links)]

    # Download each PDF if not already downloaded
    for (pdf_link in pdf_links) {
      if (!(pdf_link %in% downloaded_links)) {
        safe_name <- as.character(file_counter)
        pdf_file <- file.path(output_dir, paste0(safe_name, ".pdf"))

        tryCatch({
          download.file(pdf_link, pdf_file, mode = "wb")
          # Append metadata
          metadata <- rbind(metadata, data.frame(Link = pdf_link, Title = safe_name, stringsAsFactors = FALSE))
          downloaded_links <- c(downloaded_links, pdf_link)
          file_counter <- file_counter + 1
        }, error = function(e) {
          message("Failed to download ", pdf_link, ": ", e)
        })
      } else {
        message("Skipping already downloaded PDF: ", pdf_link)
      }
    }
  }

  # Write metadata to CSV
  write.csv(metadata, metadata_file, row.names = FALSE)
}
```

```{r warning=FALSE, message=FALSE}
# Run the function
# scrape_google_scholar("donald+trump", 20, "resources/pdf/donald+trump")
```

```{r warning=FALSE, message=FALSE}
# Run the function
# scrape_google_scholar("hillary+clinton", 20, "resources/pdf/hillary+clinton")
```

<br>

### Output

We collected more than 50+ Journal/Article PDFs about Donald Trump and Hillary Clinton

```
pdf/
└── donald+trump/
    ├── pdf_metadata.csv
    ├── <filename_1>.pdf
    ├── <filename_2>.pdf
    ├── ...
└── hillary+clinton/
    ├── pdf_metadata.csv
    ├── <filename_1>.pdf
    ├── <filename_2>.pdf
    ├── ...
```

<br>

## Data Cleaning 

<br>

### Load required libraries

```{r warning=FALSE, message=FALSE}
# Load required libraries
library(tidyverse)
library(pdftools)
library(tidytext)
library(ggplot2)
```

<br>

### Read text from scraped PDFs

```{r warning=FALSE, message=FALSE}
# Set directory containing PDFs
pdf_dir <- "resources/pdf"

# List all PDF files in the directory
pdf_files <- list.files(pdf_dir, full.names = TRUE)

# Read the PDF files into a data frame
text_data <- lapply(pdf_files, function(file) {
  tryCatch(
    {
      pdf_text(file)
    },
    error = function(e) {
      warning(paste("Error reading file:", file))
      return(NA)
    }
  )
}) %>%
  unlist() %>%
  na.omit() %>%
  data.frame(text = .)
```

<br>

### Cleaning the text data

```{r warning=FALSE, message=FALSE}
# Function to clean text data
clean_text <- function(text) {
  # Convert to lowercase
  text <- tolower(text)
  
  # Remove punctuation
  text <- gsub("[[:punct:]]", " ", text)
  
  # Remove numbers
  text <- gsub("[[:digit:]]", "", text)
  
  # Remove extra white spaces
  text <- gsub("\\s+", " ", text)
  
  # Remove stop words
  text <- removeWords(text, stopwords("en"))
  
  return(text)
}

# Apply text cleaning function to the text data
text_data <- text_data %>%
  mutate(cleaned_text = map_chr(text, clean_text))

```

<br>

## Sentiment Analysis Visualization

```{r warning=FALSE, message=FALSE}
# Perform sentiment analysis for each candidate
sentiment_analysis <- text_data %>%
  mutate(candidate = case_when(
    str_detect(cleaned_text, "donald trump") ~ "Trump",
    str_detect(cleaned_text, "hillary clinton") ~ "Clinton",
    TRUE ~ "Other"
  )) %>%
  filter(candidate != "Other") %>%
  unnest_tokens(word, cleaned_text) %>%
  inner_join(get_sentiments("bing"), by = c("word" = "word")) %>%
  group_by(candidate) %>%
  summarise(sentiment_score = sum(sentiment == "positive") - sum(sentiment == "negative"))
```

```{r warning=FALSE, message=FALSE}
# Plotting the sentiment analysis results
ggplot(sentiment_analysis, aes(x = candidate, y = sentiment_score, fill = candidate)) +
  geom_bar(stat = "identity") +
  labs(x = "Candidate", y = "Sentiment Score", title = "Sentiment Analysis for Trump vs Clinton") +
  theme_minimal() +
  scale_fill_manual(values = c("Trump" = "red", "Clinton" = "blue")) +
  geom_text(aes(label = sentiment_score), vjust = -0.5)
```

<br>

# Commentary

<br>

### Analysis Based on Tweets

The first bar chart shows the sentiment analysis scores for tweets mentioning Donald Trump and Hillary Clinton.

#### Sentiment Scores

- Donald Trump: The sentiment score is significantly higher, around 3500.
- Hillary Clinton: The sentiment score is lower, around 1500.

#### Interpretation

- Higher Sentiment Score for Trump: This could indicate that tweets mentioning Donald Trump generally have a more positive sentiment compared to those mentioning Hillary Clinton. It's also possible that Trump has a larger volume of tweets which could influence the overall sentiment score.
- Lower Sentiment Score for Clinton: Tweets mentioning Hillary Clinton tend to have a less positive or more negative sentiment compared to Donald Trump.

#### Implications

- Public Perception: Based on the data from tweets, Donald Trump appears to have a more favorable sentiment among Twitter users compared to Hillary Clinton.
- Social Media Influence: The higher volume and possibly more positive mentions of Trump on Twitter could be reflective of his stronger presence or engagement on social media platforms.

<br>

### Analysis Based on Journal Articles

The second bar chart compares the sentiment from journal articles for both candidates.

#### Sentiment Distribution

- Donald Trump:
  - Negative Sentiment: Around 6000.
  - Positive Sentiment: Slightly higher than negative, around 6200.
- Hillary Clinton:
  - Negative Sentiment: Around 6800.
  - Positive Sentiment: Slightly lower than negative, around 6400.

#### Interpretation

- Balanced Sentiment for Trump: The sentiment towards Donald Trump in journal articles is relatively balanced, with a slightly higher positive sentiment.
- Negative Sentiment for Clinton: Hillary Clinton has a higher negative sentiment in journal articles compared to positive sentiment.

#### Implications

- Academic and Media Perception: Journal articles tend to have a more balanced view of Trump, while Clinton receives more negative sentiment.
- Public Discourse: The sentiment in journal articles reflects the complexities of each candidate's public image, policies, and controversies discussed in academic and media circles.